# Feedback Loop Analysis: 2026-02-03 23:15

## Data Summary

- **Forecasts analyzed**: 47
- **Meta-reflections**: 47
- **Resolved forecasts with our predictions**: 0 (missed 4 resolved questions)
- **CP comparisons**: 16
- **Average CP divergence**: +7.2% (we forecast higher than CP)

## Ground Truth Status

**No Brier score data yet** - 4 questions resolved but we had no forecasts on them. This means CP divergence is still a weak signal - we cannot know if our divergences represent an edge or miscalibration until more questions resolve.

## Large Divergence Analysis

### Pattern 1: Meta-Predictions (3 of 4 large divergences)

| Question | Our Forecast | CP | Divergence |
|----------|-------------|-----|------------|
| Q41955 (EU age verification CP >26%) | 70% | 45% | +25% |
| Q41963 (Starmer CP >34%) | 72% | 45% | +27% |
| Q41987 (AI song CP >46%) | 78% | 45% | +33% |

**Pattern**: Agent consistently forecasts 70-78% YES for "Will CP be above X%?" questions when current CP is already above threshold.

**Agent's reasoning** (from meta-reflections):
- "Current CP already at X%, above threshold"
- "Large forecaster base (500+) creates stability"
- "Short time horizon limits opportunity for large shifts"

**The problem**: The agent treats CP stickiness as a strong prior, but the market (CP ~45%) prices in more uncertainty about CP drift.

**Root cause**: The agent underestimates:
- Random noise and forecaster churn
- Mean-reversion effects
- Uncertainty it hasn't modeled

### Pattern 2: Definitional Question (Q41906)

| Question | Our Forecast | CP | Divergence |
|----------|-------------|-----|------------|
| Q41906 (Trump admin initiate NYC funding process) | 88% | 40% | +48% |

**Agent's reasoning**:
- "NYC explicitly named on DOJ sanctuary jurisdiction list under EO 14287"
- "HHS funding freeze placed NY state on restricted drawdown"
- "Appears to qualify" / "formal documented government action"

**The problem**: This is a definitional question where the agent interpreted criteria liberally. The 88% forecast assumes:
1. These actions qualify as "initiate another process"
2. The state-level HHS action satisfies "specifically allocated to NYC"
3. The question author will interpret criteria the same way

The agent's own factors include "Definitional uncertainty" at -0.8 logits, showing awareness of interpretation risk, but the overall forecast didn't reflect this uncertainty adequately.

**Root cause**: Over-interpretation. The agent saw formal government actions and concluded they "appear to qualify" without adequately accounting for the possibility that the question author applies stricter criteria.

## Changes Made

### 1. Meta-Prediction Calibration Warning

Added to `src/aib/agent/prompts.py` in the Meta-Predictions section:

> **Calibration warning**: If the meta-question itself trades at 40-50% but you calculate 70%+ based on "CP is already above threshold", you're overestimating CP stickiness. Markets price in random noise, mean-reversion, and uncertainty you haven't modeled. When your estimate diverges >20pp from the meta-question's own market price, **trust the market** unless you have specific information about upcoming catalysts.

### 2. Definitional Question Divergence Guard

Added to `src/aib/agent/prompts.py` in the Definitional Questions section:

> **Large Divergence from CP = Red Flag**: If you're forecasting a definitional question and your probability diverges >20pp from CP, stop and ask: Am I more confident than 500+ other forecasters in my interpretation? Do I have access to information they don't? **Default rule**: For definitional questions with >20pp CP divergence, cap your confidence.

## Previous Changes (Still in Effect)

From 2026-02-03 earlier session:
- **Stock tools** (stock_price, stock_history) - Already added and exposed
- **FRED tools** (fred_series, fred_search) - Already added and exposed
- **Wikipedia redirects fix** - Already in place

## Validation Plan

| Change | How to Validate | When |
|--------|-----------------|------|
| Meta-prediction calibration | Agent anchors more on meta-question CP | Next forecasts |
| Definitional divergence guard | Agent reduces forecast when >20pp divergence | Next forecasts |
| Overall calibration | Brier scores from resolutions | Feb 15+ |

## Next Steps

1. **Wait for resolutions** (Feb 15) - Q41955, Q41963, Q41987, Q41906
2. **Re-run feedback loop** with Brier score data to validate whether prompt changes helped
3. **Monitor tool failures** - TradingEconomics still failing, may need dedicated tool

## Commits

1. `meta(feedback): add calibration warning for meta-predictions` - Trust market when >20pp divergence
2. `meta(feedback): add definitional question divergence guard` - Cap confidence when interpretation uncertain
