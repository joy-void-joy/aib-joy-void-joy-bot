# Deep Analysis: Prompts, Meta-Reflections, and Architecture

**Date**: 2026-02-02
**Reviewed**: 29 forecasts, 29 meta-reflections

---

## Part 1: Prompt Effectiveness

### What's Working Well

1. **"Nothing Ever Happens" Calibration**
   - Consistently applied across forecasts
   - Meta-reflections show explicit reasoning about status quo bias
   - Example: Russia-Ukraine ceasefire → "Ceasefire would be dramatic and highly newsworthy... -0.5 to -1.0 logits"

2. **Factor Decomposition**
   - All forecasts include structured factors with logit values
   - Confidence weighting is being used appropriately
   - Creates audit trail for reasoning

3. **Resolution Criteria Checklist**
   - Agents are parsing criteria carefully
   - Edge cases are being identified
   - Example: Trump NYC funding → explicit uncertainty about "state vs city" targeting

### What's Missing or Weak

1. **Question Type Differentiation**
   - "Nothing Ever Happens" doesn't fit all question types
   - Meta-reflection explicitly noted: "less applicable to questions about whether past actions qualify under criteria"
   - **Gap**: No guidance for "definitional" questions (has X happened?) vs "predictive" questions (will X happen?)

2. **Google Trends Questions**
   - Multiple forecasts on Google Trends but no specific guidance
   - Agents note they can't verify current values or access historical patterns
   - **Gap**: No guidance on search interest decay patterns, no tool access

3. **Meta-Questions (Predictions about Predictions)**
   - Questions like "Will community prediction be >26%?" require different reasoning
   - Need to understand how forecaster consensus moves, not underlying events
   - **Gap**: No guidance on forecasting Metaculus community predictions

4. **Subagent Triggering**
   - Prompt mentions subagents but doesn't specify when to use them
   - Result: 0/29 forecasts used subagents
   - **Gap**: No explicit criteria for "this is complex enough to spawn"

5. **Market Integration**
   - Markets are consulted but integration is ad-hoc
   - No guidance on how to weight by volume/liquidity
   - No guidance on interpreting market disagreements

### Prompt Recommendations

```markdown
## Question Type Classification

Before researching, classify the question:

| Type | Description | Key Approach |
|------|-------------|--------------|
| **Predictive** | "Will X happen by Y?" | Base rate + updates + "Nothing Ever Happens" |
| **Definitional** | "Has X happened? Does Y qualify?" | Resolution criteria parsing, not prediction |
| **Meta-prediction** | "Will prediction/price be at Z?" | Model the forecaster, not the underlying |
| **Measurement** | "What will value of X be?" | Current value + drift + volatility |

"Nothing Ever Happens" applies strongly to Predictive questions, weakly to Measurement questions, and not at all to Definitional questions.

## Google Trends Questions

For questions about Google Trends values:
1. The question provides a starting value - use it as your anchor
2. Search interest follows predictable patterns:
   - Peaks during active news events
   - Decays exponentially after peak (half-life ~3-7 days)
   - Rebounds before scheduled events (elections, deadlines)
3. ±3 point thresholds are sensitive - current values matter enormously
4. Search for upcoming events that could spike interest

## Meta-Predictions (Forecasts about Forecasts)

For questions like "Will CP be above X% on date Y?":
1. Check current value and recent trend
2. Large forecaster bases (500+) are sticky - hard to move significantly
3. New information affects both underlying probability AND forecaster consensus
4. Markets with low liquidity/forecasters are more volatile
5. The underlying event probability informs but doesn't determine CP movement

## When to Use Subagents

**Spawn a subagent when:**
- Research would benefit from 3+ distinct searches
- Question has multiple independent factors
- You need a base rate calculation
- The question spans multiple domains
- You're uncertain where to start (use quick-researcher)

**Default to main thread when:**
- Single recent news search answers the question
- You already know the key facts
- The question is definitional (does X qualify?)
- Time pressure requires fast turnaround

## Market Price Integration

| Volume | Treatment |
|--------|-----------|
| High (Polymarket >$10k, Manifold >1000 traders) | Strong signal - anchor on this |
| Medium | Useful sanity check |
| Low (<$1k, <50 traders) | Single data point, don't over-weight |

When markets disagree significantly (>20pp):
1. Check if they're asking slightly different questions
2. Check if one has much higher volume
3. Note the disagreement as uncertainty
```

---

## Part 2: Meta-Reflection Template Quality

### What's Being Captured

- ✅ Research strategy and sources
- ✅ Tool effectiveness assessment
- ✅ "Nothing Ever Happens" application
- ✅ System design suggestions
- ✅ Update triggers

### What's Missing

1. **No Post-Hoc Reasoning Critique**
   - Meta-reflections don't question their own reasoning
   - No "what might I be missing?" or "steel-man the opposite"
   - **Add**: "What's the strongest argument against my forecast?"

2. **No Quantitative Calibration Tracking**
   - Reflections say "moderate confidence" but no numbers
   - No tracking of similar past forecasts for calibration
   - **Add**: "How confident am I really? (68% CI: [X, Y])"

3. **No Time Spent Tracking**
   - Don't know if 2-hour research vs 20-minute research
   - Can't optimize research depth
   - **Add**: "Approximate research time and tool calls"

4. **Formulaic System Design Section**
   - Same structure every time
   - "Tool gaps" → mentions missing tool → moves on
   - **Add**: More specific prompts about what went wrong

5. **No Subagent Decision Justification**
   - Every reflection says "Didn't use subagents"
   - Never explains why not, or considers if they should have
   - **Add**: "Should I have used subagents? Why or why not?"

### Revised Meta-Reflection Template

```markdown
## REQUIRED: Meta Reflection

Write to `notes/meta/<timestamp>_q<id>_<slug>.md`. This reflection improves future forecasting.

### 1. Forecast Summary (2-3 sentences)
- Question, final forecast, confidence level (with numeric 80% CI if possible)

### 2. Research Audit
- What searches did you run? Which were valuable?
- What sources were most informative?
- What did you try that didn't work?
- Approximate research time: ___ minutes, ___ tool calls

### 3. Reasoning Quality Check
**Strongest evidence for my forecast:**
1. ...
2. ...

**Strongest argument AGAINST my forecast:**
- What would a smart person who disagrees say?
- What evidence would change my mind?

**"Nothing Ever Happens" check:**
- Is this question predictive, definitional, or measurement?
- Did I apply appropriate skepticism for the question type?

### 4. Subagent Decision
- Did I use subagents? If not, should I have?
- For complex questions: Why didn't I decompose this?

### 5. Calibration Notes
- Numeric confidence: "I'm 80% confident the true value is between X and Y"
- What comparable forecasts have I made? How did they resolve?
- What would make me update significantly?

### 6. System Feedback
Be specific about what went wrong, not just what's missing:
- Tool that failed or returned unhelpful results
- Prompt guidance that didn't match this question type
- Capability I needed but didn't have

### 7. If I Did This Again
- What would I do differently with the same tools?
- What would I do differently with ideal tools?
```

---

## Part 3: Architecture Assessment

### Subagent Utilization Problem

**Observation**: 0/29 forecasts used any subagent.

**Why this is happening:**
1. Main agent can do everything subagents can do
2. No clear criteria for when to spawn
3. Subagent overhead (context switching, JSON parsing) feels high
4. Questions are being treated as "not complex enough"

**But some questions clearly warranted subagents:**
- Sinners Oscars (16 categories to analyze)
- S&P 500 (multiple scenarios to model)
- Russia-Ukraine ceasefire (multiple diplomatic tracks to research)

**Recommendation**: Either:
1. Make subagent spawning automatic for certain question types
2. Add explicit criteria in prompt for "spawn when X"
3. Restructure subagents to be lighter-weight

### Tool Gaps (Prioritized by Frequency)

| Tool | Times Mentioned | Impact |
|------|-----------------|--------|
| Google Trends API | 6 | High - multiple questions about Trends |
| Real-time financial data | 5 | High - stock/index questions |
| Better WebFetch (JS rendering) | 4 | Medium - frustrating but workaround exists |
| FRED historical data | 3 | Medium - helps with economic forecasts |
| Prediction market aggregator | 3 | Medium - current tools partially work |
| Official government docs | 2 | Low - rare question type |

### Current Subagent Issues

1. **quick-researcher (Haiku)**: Never used. Either:
   - Make it default for initial orientation
   - Remove it entirely

2. **deep-researcher (Sonnet)**: Never used. Issues:
   - Prompt is generic "flexible research"
   - Overlaps entirely with main agent capability
   - No clear trigger condition

3. **link-explorer (Haiku)**: Never used. Issues:
   - Markets tool often fails
   - Metaculus coherence links often 404
   - Could be folded into main agent

4. **fact-checker (Haiku)**: Never used. Issues:
   - Unclear when this adds value
   - Main agent already cross-checks

5. **estimator (Sonnet)**: Never used. Issues:
   - Main agent can run execute_code itself
   - No clear benefit to spawning

**Recommendation**: Consolidate to 2-3 higher-value subagents or make spawning automatic.

### Architecture Recommendations

1. **Auto-spawn quick-researcher** at question start for orientation
2. **Merge** link-explorer into main agent tools
3. **Restructure** deep-researcher to be domain-specific:
   - `financial-researcher` (stock prices, economic data)
   - `event-researcher` (geopolitics, policy)
   - `measurement-researcher` (Google Trends, metrics)
4. **Add tools** not subagents for:
   - Google Trends API
   - Financial data API
   - FRED historical data

---

## Part 4: Changes Implemented

### Prompt Changes (prompts.py) ✅

1. ✅ Added "Question Type Classification" section (predictive/definitional/meta/measurement)
2. ✅ Added "Google Trends Questions" guidance
3. ✅ Added "Meta-Predictions" guidance
4. ✅ Added "When to Use Subagents" criteria with explicit triggers
5. ✅ Added "Market Price Integration" weighting guidance
6. ✅ Added "When Nothing Ever Happens Does NOT Apply" section
7. ✅ Revised meta-reflection template:
   - Added "Strongest argument AGAINST my forecast" requirement
   - Added numeric 80% CI requirement
   - Added subagent decision justification
   - Simplified and focused on specifics over formulaic sections

### Feedback Loop Command Changes ✅

1. ✅ Restructured to analyze two lenses: calibration AND process quality
2. ✅ Added meta-reflection quality review section
3. ✅ Added "Quick Reference: What to Look For" pattern-matching guide
4. ✅ Added subagent utilization tracking
5. ✅ Added tool failure pattern tracking
6. ✅ Prioritized prompt changes over tool changes

### Changes Deferred (Future Work)

**Subagent Changes (subagents.py)**
- Consider: Remove or merge underused subagents
- Consider: Make quick-researcher spawn automatically at start
- Rationale: Need to see if new prompt guidance improves utilization first

**Tool Changes (tools/)**
- Priority: Add Google Trends data tool
- Priority: Add financial data tool (Yahoo Finance API)
- Priority: Fix Polymarket/Manifold search reliability
- Rationale: Higher effort, want to see prompt impact first

### Script Changes ✅

1. ✅ Fixed feedback_collect.py to use local metaculus package
2. ✅ Added "missed questions" reporting to feedback collection
