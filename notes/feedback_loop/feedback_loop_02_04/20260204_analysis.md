# Feedback Loop Analysis: 2026-02-04

## Ground Truth Status

- **Resolved forecasts with valid predictions**: 0
- **Average Brier score**: N/A (no valid data)
- **Note**: The forecast for 41835 (government shutdown) was made AFTER the question resolved (forecast: Feb 4, resolution: Jan 31). This was incorrectly included in calibration metrics. Script bug identified and fixed.

Since we have no resolution data, this analysis focuses on **process quality** and **tool failures**.

## Object-Level Findings

### Tool Failures

| Tool | Failure Mode | Count | Fix |
|------|--------------|-------|-----|
| `get_cp_history` | 100% error rate | 3 | Agent using `post_id` instead of `question_id` - tool documentation unclear |
| `search_metaculus` | `conditional` question type | 15 | Library doesn't support conditional questions |
| Meta-reflection missing | Agent exits without writing | 215 | Agent not following reflection requirements |

### Agent Capability Requests

From log trace analysis:
- "CP history didn't work - I need to use the correct question_id" (agent recognized issue but couldn't resolve)
- No Polymarket/Manifold markets for many questions - agent adapts by using other signals

### Reasoning Quality Assessment

**Reviewed forecasts (5 with largest CP divergence):**

1. **41835 (Gov shutdown, 97% vs CP 25%, +72pp)** - INVALID: Post-resolution forecast, not genuine prediction. Removed from data.

2. **41906 (Trump admin NYC funding, 88% vs CP 40%, +48pp)**
   - Reasoning: Sound - agent identified multiple formal government actions (DOJ sanctuary list, HHS funding freeze)
   - Correctly noted definitional uncertainty about "specifically allocated to NYC"
   - Assessment: Good reasoning, divergence may reflect edge from detailed analysis

3. **41987 (AI song meta-prediction, 78% vs CP 45%, +33pp)**
   - Reasoning: Sound - agent correctly identified current CP at 50% (above 46% threshold)
   - Noted 866 forecasters = sticky prediction, short time horizon
   - Used Manifold (36%) as comparison signal but correctly weighted Metaculus forecaster behavior
   - Assessment: Excellent reasoning for meta-prediction dynamics

4. **41963 (Starmer meta-prediction, 72% vs CP 45%, +27pp)**
   - Reasoning: Sound - CP already at 35% (above 34% threshold)
   - Correctly analyzed political turmoil, Manifold higher at 40-42%
   - Assessment: Good reasoning, correctly weighted "staying above threshold" vs "threshold exact"

5. **41955 (EU age verification meta-prediction, 70% vs CP 45%, +25pp)**
   - Reasoning: Sound - CP at 28% (above 26% threshold), upward trend
   - 770 forecasters providing stability
   - Assessment: Good reasoning

**Overall**: Reasoning quality is high. Large CP divergences appear to reflect genuine analysis differences, not errors. Many are meta-predictions where the agent correctly identifies that current CP is already above threshold.

## Meta-Level Findings

### Meta-Reflection Quality

**Critical issue**: 215 forecasts are MISSING meta-reflections entirely. The agent is not following the required reflection step.

- Agent produces good inline reasoning (visible in logs)
- But fails to write formal meta-reflection to `notes/meta/`
- This blocks structured feedback collection

**Recommendation**: Simplify meta-reflection requirements or make them automatic post-processing.

### Tracking Issues

1. **feedback_collect.py bug**: Doesn't filter forecasts by timestamp relative to resolution. Fixed in this session.

2. **get_cp_history tool**: Requires `question_id` but agent often only has `post_id`. Tool description mentions this but agent still gets confused.

### Template Improvements Needed

The meta-reflection template isn't being used. Before improving the template, need to ensure the agent actually writes reflections.

## Meta-Meta Findings

### What data did I lack?

- **Programmatic tool call tracing**: Logs show metrics but not individual tool call inputs/outputs
- **Question ID vs Post ID mapping**: Would help debug `get_cp_history` failures
- **Resolution timestamps at forecast time**: feedback_collect didn't validate timing

### Scripts to build or improve

1. **feedback_collect.py**: Filter forecasts made after resolution (FIXED)
2. **tool_trace.py**: Extract individual tool calls from logs with inputs/outputs
3. **id_mapping.py**: Helper to convert between question_id and post_id

### Process improvements

1. The "large CP divergence" metric is misleading for meta-predictions. Meta-predictions naturally have different dynamics - the agent forecasts whether CP will be above a threshold, while CP of the meta-question reflects crowd belief about that. These aren't directly comparable.

2. Need to track meta-prediction vs. object-level prediction separately in metrics.

## Changes Made

| Level | Change | Rationale |
|-------|--------|-----------|
| Object | Fixed feedback_collect.py to filter post-resolution forecasts | Script bug included invalid calibration data |
| Object | Removed 41835 forecast directory | Post-resolution "forecast" not a valid data point |
| Meta | Documented meta-reflection gap | 215 missing reflections - major process issue |
| Process | Identified meta-prediction tracking gap | CP divergence misleading for this question type |

## Next Actions

1. **Fix feedback_collect.py** - Add timestamp validation before including in calibration
2. **Investigate meta-reflection failures** - Why isn't agent writing reflections?
3. **Improve get_cp_history documentation** - Make question_id requirement clearer
4. **Wait for resolutions** - Real calibration data needed before drawing accuracy conclusions
