# Feedback Loop Analysis: 2026-02-02 (Follow-up)

This is a follow-up analysis after prompt improvements were made in commit `4535281`. The previous analysis (`20260202_220000_deep_analysis.md`) identified patterns and led to prompt changes. This analysis assesses:
1. Whether the changes are being applied
2. New patterns since the changes
3. Early calibration signals from CP comparison

## Data Summary

- **Forecasts analyzed**: 33 (in `notes/forecasts/`)
- **Meta-reflections**: 31 (in `notes/meta/`)
- **Resolved forecasts**: 0 (no calibration data yet)
- **Missed questions**: 4 (resolved without our forecast)
- **Community prediction comparisons**: 10

## Status of Previous Improvements

| Improvement | Status | This Iteration |
|-------------|--------|----------------|
| Question Type Classification | ✅ In prompts | No change |
| Google Trends Guidance | ✅ Being used | No change |
| Meta-Prediction Guidance | ✅ Applied to Q41955, Q41963 | No change |
| Subagent Triggering Criteria | ⚠️ Zero usage | **Clarified**: Zero usage may be correct |
| Market Integration Guidance | ✅ Being used | No change |
| Meta-Reflection Self-Critique | ⚠️ Inconsistent | No change |
| Definitional Question Guidance | ❌ Minimal | **Added**: Full section |

## Key Findings

### 1. Community Prediction Divergence Analysis

**Average divergence**: +8.4% (we forecast higher than CP on average)

**Large divergences (>15pp)**:
| Question | Our Forecast | CP | Divergence | Type |
|----------|-------------|-----|------------|------|
| Q41906 (Trump federal employee process) | 88% | 40% | +48pp | Definitional |
| Q41963 (Starmer CP meta-question) | 72% | 45% | +27pp | Meta-prediction |
| Q41955 (EU age verification CP) | 70% | 45% | +25pp | Meta-prediction |

**Pattern identified**: Two of the three largest divergences are **meta-predictions about CP movement**. Both forecasts essentially bet that "CP will stay where it is or rise" when CP is already at/above threshold. This may indicate:
1. Correct reasoning (CP is already above threshold → likely stays)
2. OR systematic overconfidence in predicting CP stability

**Q41906 divergence (+48pp)** is the most concerning. Reading the meta-reflection, this is a **definitional question** ("has X happened?") where we assigned 88% because actions "appear to qualify." The 48pp divergence from CP suggests either:
- We have information the crowd doesn't (possible - thorough research)
- We're misinterpreting resolution criteria (risky)
- We're overconfident on definitional edge cases

### 2. Subagent Utilization: Zero Usage

**Critical finding**: Across all 31 meta-reflections, **no subagents were spawned**. Every reflection includes a variant of:
> "Didn't use subagents for this relatively straightforward question"

This is concerning because:
1. Some questions (Sinners Oscars, EU HY spreads) had 5+ factors and involved Monte Carlo simulations
2. The prompt explicitly recommends spawning quick-researcher for unfamiliar topics
3. Deep-researcher is recommended for base rate calculations with historical data

**Why no subagents?** Likely causes:
- The "bar" is perceived as too high ("only for complex questions")
- Coordination overhead feels expensive for short time windows
- Direct research felt sufficient in the moment
- No clear trigger criterion in practice

### 3. Tool Effectiveness Patterns

**Consistently valuable**:
- `WebSearch` - mentioned as "most valuable" in majority of reflections
- `execute_code` - essential for Monte Carlo simulations and statistical analysis
- `mcp__forecasting__get_metaculus_questions` - critical for question details

**Consistently problematic**:
- `WebFetch` - frequently fails on JavaScript-heavy sites (Variety, IndieWire, TradingEconomics)
- `polymarket_price`/`manifold_price` - often return "no matching markets"
- `get_coherence_links` - frequent 404 errors

**Tool gaps mentioned repeatedly**:
1. **Google Trends API** - mentioned in 2+ reflections
2. **FRED historical data** - can't download time series, only view current page
3. **CP history tracking** - no way to see historical community prediction movement
4. **Official government document search** (Federal Register, OMB)
5. **Better JavaScript rendering** for WebFetch

### 4. "Nothing Ever Happens" Application

**Applied appropriately** in most cases:
- Q41908 (H5N1 PHEIC): Applied -0.5 logits for dramatic outcome → 6% final
- Sinners Oscars: Applied moderately, skeptical of "sweep" scenario

**Misapplied or not applicable**:
- Q91 (Trump NYC funding): Reflection correctly notes NEH doesn't apply to definitional questions
- Meta-predictions: Unclear how to apply - forecasting CP movement, not events

**Gap identified**: The prompt doesn't clearly distinguish how calibration differs for definitional vs predictive questions. Definitional questions ("does X qualify?") require resolution criteria parsing, not status quo skepticism.

### 5. Meta-Reflection Quality

**Strong patterns**:
- Consistent structure across all reflections
- Genuine self-critique in "Key uncertainties" sections
- Specific update triggers identified
- Tool feedback is detailed and actionable

**Weak patterns**:
- "Subagent usage" section is formulaic (always "didn't use, question was straightforward")
- Numeric confidence intervals not always provided
- Comparable forecasts section often empty ("none in my history")

## Changes Made This Iteration

### 1. Subagent Guidance: Clarified When Useful (`src/aib/agent/prompts.py`)

Analyzed whether subagents would actually help. Conclusion: **zero usage might be correct**.

The main agent already has all the same tools as subagents. Subagents only help when:
- 3+ truly independent research threads that don't inform each other
- Coordination overhead < parallelization benefit
- Each thread is substantial enough to warrant a separate agent

Most forecasting questions are simple enough that subagents add overhead without benefit. Updated guidance to reflect this reality rather than forcing usage.

**Key insight**: Don't use subagents just because they exist - use them when they genuinely help.

### 2. ~~Large Divergence Warning~~ REMOVED

Initially added a section warning about CP divergence. **Removed** because:
- **The forecasting agent cannot see community predictions** during forecasting
- CP is only available to the feedback loop after the forecast is made (AIB tournament rule)
- This section was not actionable during forecasting

CP comparison remains useful for the feedback loop (post-hoc calibration analysis) but not for the agent.

### 3. Definitional Question Guidance (`src/aib/agent/prompts.py`)

Added dedicated section for "Has X happened? Does Y qualify?" questions:
- Exhaustive criteria parsing approach
- Common pitfalls (over-interpretation, missing qualifiers)
- Warning about interpretation edge cases

**Rationale**: Largest CP divergence was on definitional question. Better criteria parsing guidance is actionable.

### 4. Feedback Loop Command (`.claude/commands/feedback-loop.md`)

Restructured to emphasize implementation:
- Renamed "Analyze and Improve" → "Analyze, Diagnose, and Fix"
- Added explicit "Phase 4: Implement Fixes" with code examples
- Clarified that CP comparison is **post-hoc only** (agent can't see CP)
- Added diagnosis guide table
- Added example workflow with actual commands

**Rationale**: Previous iterations documented problems but didn't make code changes.

## Issues Status

### ✅ Analyzed: Subagent Adoption Zero → Probably Correct

Previous concern: Zero subagent usage despite advisory guidance.

**Analysis**: The main agent has access to ALL the same tools as subagents. Subagents only help when:
- 3+ truly independent research threads
- Coordination overhead < parallelization benefit

Most questions are simple enough that subagents add overhead without benefit. **Zero usage may be correct behavior, not a problem.**

Updated guidance to clarify when subagents genuinely help vs. add overhead.

### ⏳ Monitoring: Meta-Prediction CP Divergence

Q41963 (Starmer CP): 72% vs 45% CP (+27pp)
Q41955 (EU age verification CP): 70% vs 45% CP (+25pp)

Our reasoning is defensible (CP is above threshold → likely stays). Will validate when both resolve Feb 15.

### ✅ Addressed: Q41906 Definitional Divergence (+48pp)

Added comprehensive "Definitional Questions: Criteria Parsing" section with:
- Exhaustive criteria parsing approach
- Common pitfalls (over-interpretation, missing qualifiers)
- Warning about interpretation edge cases

**Note**: Cannot add "CP divergence warning" to agent prompts because agent cannot see CP during forecasting. CP comparison is only available post-hoc in the feedback loop.

### ⏸️ Deferred: Tool Failures

WebFetch JavaScript issues, market API "no matching" responses remain unaddressed. Will address if pattern persists after prompt changes prove insufficient.

## Calibration Observations

Without resolved forecasts, we can only analyze process quality and CP divergence:

1. **Average +8.4% divergence from CP** suggests slight optimism bias OR systematic edge
2. **2/3 largest divergences are meta-questions** - may indicate we model CP stability better than crowd
3. **Q41906 (+48pp)** is an outlier worth monitoring - definitional question with high confidence

## Validation Plan

| Change Made | How to Validate | When |
|-------------|-----------------|------|
| Subagent guidance clarification | N/A - guidance now matches reality | - |
| Definitional question guidance | Track definitional Q divergence from CP | Next feedback loop |
| Feedback-loop command restructure | Did this iteration make code changes? | ✅ Yes |

## Next Steps

1. **Run forecasts with new prompts** - See if subagent usage increases
2. **Monitor Feb 15 resolutions** - Q41963, Q41955 (meta-predictions), possibly Q41906
3. **Re-run feedback loop after resolutions** - Need Brier scores to assess calibration
4. **Address tool failures if needed** - If WebFetch/market API issues persist

## Comparison to Previous Analysis

| Metric | Previous (20260202_220000) | This Iteration |
|--------|---------------------------|----------------|
| Forecasts analyzed | 29 | 33 |
| Meta-reflections | 29 | 31 |
| Subagent usage | 0/29 | 0/31 (analyzed: may be correct) |
| Prompt improvements | Identified | ✅ Refined |
| Resolved forecasts | 0 | 0 |
| CP comparisons | Not tracked | 10 |
| Code changes made | No | ✅ Yes |

**Key progress this iteration**:
- Analyzed whether subagent usage should increase (conclusion: probably not)
- Added definitional question guidance (actionable during forecasting)
- Clarified that CP comparison is post-hoc only (agent can't see CP)
- Restructured feedback-loop command to require implementation

**Key insight**: Not every identified "issue" needs fixing. Zero subagent usage appeared problematic but upon analysis is probably correct behavior.
